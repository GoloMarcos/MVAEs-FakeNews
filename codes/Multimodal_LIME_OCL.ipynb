{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Multimodal_LIME_OCL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_prA6mXuZaNh"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvu84C6DvTyA"
      },
      "source": [
        "!pip install git+https://github.com/GoloMarcos/FKTC/\n",
        "\n",
        "from FakeNewsTextCollections import datasets\n",
        "\n",
        "datasets_dictionary = datasets.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REHifcn-voso"
      },
      "source": [
        "fcn = datasets_dictionary['fcn']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROU359E4aCof"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = fcn[(fcn['class'] == 1) & (fcn['fold'] == 0)]\n",
        "test = fcn[(fcn['class'] == 1) & (fcn['fold'] != 0)]\n",
        "outlier = fcn[fcn['class'] == -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628RaKxyZ0s7"
      },
      "source": [
        "# One-Class Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEqdDTJuxBHZ"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluation_one_class(preds_interest, preds_outliers):\n",
        "  y_true = [1]*len(preds_interest) + [-1]*len(preds_outliers)\n",
        "  y_pred = list(preds_interest)+list(preds_outliers)\n",
        "  return classification_report(y_true, y_pred, output_dict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emd9dD9Aao7l"
      },
      "source": [
        "# BERTs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0cFk5QFkuNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bc1d02-f477-4064-bd91-a431b0a78b2a"
      },
      "source": [
        "!pip install sentence-transformers==1.0.4 #version used in the fake news collections"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==1.0.4\n",
            "  Downloading sentence-transformers-1.0.4.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 11.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==1.0.4) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers==1.0.4) (3.10.0.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (21.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (3.3.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==1.0.4) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers==1.0.4) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==1.0.4) (3.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.4-py3-none-any.whl size=114306 sha256=3df849739fc3d8968c8999ee1d6fc53d261594567841fd38e039b2e9657c1c33\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/29/9e/1d73f2751adb27f61c414d2656cef08c8353b9ad7cb7f149ef\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-1.0.4 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qOdFj3Gk0Om"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IGDEjRlce8P"
      },
      "source": [
        "def sentence_embedding(txts):\n",
        "\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
        "\n",
        "  sentences =[]\n",
        "\n",
        "  for txt in txts:\n",
        "    txt.replace('\\\\\\\\t', ' ')\n",
        "    txt.replace('\\\\\\\\r', ' ')\n",
        "    txt.replace('\\\\\\\\n',' ')\n",
        "    sentences.append(txt)\n",
        "\n",
        "  sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "  return sentence_embeddings "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uif5IHwp5PY9"
      },
      "source": [
        "# Density Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjQdyicG5SOq"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "def make_density_information(cluster_list, df_train, df_test, df_outlier):\n",
        "    l_x_train = []\n",
        "    l_x_test = []\n",
        "    l_x_outlier = []\n",
        "\n",
        "    len_train = len(df_train)\n",
        "    len_test = len(df_test)\n",
        "    len_out = len(df_outlier)\n",
        "\n",
        "    for cluster in cluster_list:\n",
        "        kmeans = KMeans(n_clusters=cluster, random_state=0).fit(df_train)\n",
        "\n",
        "        x_train_temp = silhouette_samples(df_train, kmeans.labels_).reshape(len_train, 1)\n",
        "        l_x_train.append(x_train_temp)\n",
        "\n",
        "        x_test_temp = silhouette_samples(np.concatenate([df_train, df_test]), np.concatenate([kmeans.labels_, kmeans.predict(df_test)])).reshape(len_train + len_test, 1)\n",
        "        l_x_test.append(x_test_temp[len_train:])\n",
        "\n",
        "        x_outlier_temp = silhouette_samples(np.concatenate([df_train, df_outlier]),  np.concatenate([kmeans.labels_, kmeans.predict(df_outlier)])).reshape(len_train + len_out, 1)\n",
        "        l_x_outlier.append(x_outlier_temp[len_train:])\n",
        "\n",
        "    return np.concatenate(l_x_train, axis=1), np.concatenate(l_x_test, axis=1), np.concatenate(l_x_outlier, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NgTEI3h5gAN"
      },
      "source": [
        "def return_density_inf(df_train, df_new):\n",
        "    l_x_new = []\n",
        "\n",
        "    len_train = len(df_train)\n",
        "    len_new = len(df_new)\n",
        "\n",
        "    for cluster in cluster_list:\n",
        "        kmeans = KMeans(n_clusters=cluster, random_state=0).fit(df_train)\n",
        "        x_new_temp = []\n",
        "        for example in df_new:\n",
        "          example = example.reshape(1,512)\n",
        "          dfs = np.concatenate([df_train, example])\n",
        "          labels = np.concatenate([kmeans.labels_, kmeans.predict(example)])\n",
        "          \n",
        "          silho = silhouette_samples(dfs, labels)[len_train:]\n",
        "          x_new_temp.append(silho)\n",
        "\n",
        "        l_x_new.append(x_new_temp)\n",
        "\n",
        "    return np.concatenate(l_x_new, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb92-CDJD6jB"
      },
      "source": [
        "# LIWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPa8lp5RIabX",
        "outputId": "c23c7b92-38b7-4ad6-83b1-2af8081d814e"
      },
      "source": [
        "!gdown --id 1ybt-bi6H0gAHL0fQNaleDEtlCwmBeyPn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ybt-bi6H0gAHL0fQNaleDEtlCwmBeyPn\n",
            "To: /content/LiwcFeatures.zip\n",
            "100% 8.57M/8.57M [00:00<00:00, 23.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWVX-wOrD-yi"
      },
      "source": [
        "!unzip LiwcFeatures.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8rUqdPaECrH"
      },
      "source": [
        "from liwc.liwc import Liwc\n",
        "liwc = Liwc('dictionaries/LIWC2007_Portugues_win.dic')\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2XUPEC0Eqyt"
      },
      "source": [
        "def return_LIWC(textual_documents):\n",
        "  \n",
        "  global dict_types\n",
        "\n",
        "  dict_types = {}\n",
        "\n",
        "  for i in range(len(textual_documents)):\n",
        "    txt = str(textual_documents[i]) \n",
        "    dict_liwc = liwc.parse(txt.split(' '))\n",
        "    if dict_liwc == collections.Counter():\n",
        "      dict_liwc = {'cogmech' : 0.0}\n",
        "    dict_types[i] = dict_liwc\n",
        "\n",
        "  global data_features\n",
        "  data_features = pd.DataFrame.from_dict(dict_types, orient='index').fillna(0)\n",
        "\n",
        "  x = data_features.values\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  data_normalized = pd.DataFrame(x_scaled, index=data_features.index, columns=data_features.columns)\n",
        "\n",
        "  return data_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yaQDvC74fKc"
      },
      "source": [
        "# TripleVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWgMLtLy46o8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Input, concatenate, multiply, average, subtract, add, maximum, minimum\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs3ep1ks4iUp"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), seed=1)\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class TVAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, factor_multiply_embedding, factor_multiply_density, factor_multiply_liwc,\n",
        "                 **kwargs):\n",
        "        super(TVAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.factor_multiply_embedding = factor_multiply_embedding\n",
        "        self.factor_multiply_density = factor_multiply_density\n",
        "        self.factor_multiply_liwc = factor_multiply_liwc\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder((data[0], data[1], data[2]))\n",
        "\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            embedding_loss = tf.reduce_mean(\n",
        "                keras.losses.mean_squared_error(data[0], reconstruction[0])\n",
        "            )\n",
        "\n",
        "            embedding_loss *= self.factor_multiply_embedding\n",
        "\n",
        "            density_loss = tf.reduce_mean(\n",
        "                keras.losses.mean_squared_error(data[1], reconstruction[1])\n",
        "            )\n",
        "\n",
        "            density_loss *= self.factor_multiply_density\n",
        "\n",
        "            liwc_loss = tf.reduce_mean(\n",
        "                keras.losses.mean_squared_error(data[2], reconstruction[2])\n",
        "            )\n",
        "\n",
        "            liwc_loss *= self.factor_multiply_liwc\n",
        "\n",
        "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            kl_loss = tf.reduce_mean(kl_loss)\n",
        "            kl_loss *= -0.5\n",
        "            total_loss = embedding_loss + density_loss + liwc_loss + kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        return {\n",
        "            \"total loss\": total_loss,\n",
        "            \"embedding loss\": embedding_loss,\n",
        "            \"density loss\": density_loss,\n",
        "            \"liwc loss\": liwc_loss,\n",
        "            \"kl loss\": kl_loss\n",
        "        }\n",
        "\n",
        "\n",
        "def encoder_tvae(arq, embedding_dim, density_dim, liwc_dim, operator):\n",
        "    embedding_inputs = keras.Input(shape=(embedding_dim,), name='first_input_encoder')\n",
        "    density_inputs = keras.Input(shape=(density_dim,), name='second_input_encoder')\n",
        "    liwc_inputs = keras.Input(shape=(liwc_dim,), name='third_input_encoder')\n",
        "\n",
        "    l1 = Dense(np.max([embedding_dim, density_dim, liwc_dim]), activation='linear')(embedding_inputs)\n",
        "    l2 = Dense(np.max([embedding_dim, density_dim, liwc_dim]), activation='linear')(density_inputs)\n",
        "    l3 = Dense(np.max([embedding_dim, density_dim, liwc_dim]), activation='linear')(liwc_inputs)\n",
        "\n",
        "    fusion = None\n",
        "    if operator == 'concatenate':\n",
        "        fusion = concatenate([l1, l2, l3])\n",
        "    if operator == 'multiply':\n",
        "        fusion = multiply([l1, l2, l3])\n",
        "    if operator == 'average':\n",
        "        fusion = average([l1, l2, l3])\n",
        "    if operator == 'subtract':\n",
        "        fusion = subtract([l1, l2])\n",
        "        fusion = subtract([fusion, l3])\n",
        "    if operator == 'add':\n",
        "        fusion = add([l1, l2, l3])\n",
        "    if operator == 'max':\n",
        "        fusion = maximum([l1, l2])\n",
        "    if operator == 'min':\n",
        "        fusion = minimum([l1, l2])\n",
        "\n",
        "    if len(arq) == 3:\n",
        "        first_dense = Dense(arq[0], activation=\"linear\")(fusion)\n",
        "\n",
        "        second_dense = Dense(arq[1], activation=\"linear\")(first_dense)\n",
        "\n",
        "        z_mean = layers.Dense(arq[2], name=\"Z_mean\")(second_dense)\n",
        "        z_log_var = layers.Dense(arq[2], name=\"Z_log_var\")(second_dense)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    elif len(arq) == 2:\n",
        "        first_dense = Dense(arq[0], activation=\"linear\")(fusion)\n",
        "\n",
        "        z_mean = layers.Dense(arq[1], name=\"Z_mean\")(first_dense)\n",
        "        z_log_var = layers.Dense(arq[1], name=\"Z_log_var\")(first_dense)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    else:  # len(arq) == 1\n",
        "        z_mean = layers.Dense(arq[0], name=\"Z_mean\")(fusion)\n",
        "        z_log_var = layers.Dense(arq[0], name=\"Z_log_var\")(fusion)\n",
        "        z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    encoder = keras.Model([embedding_inputs, density_inputs, liwc_inputs], [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def decoder_tvae(arq, embedding_dim, density_dim, liwc_dim):\n",
        "    latent_inputs = keras.Input(shape=(arq[(len(arq) - 1)],), name='input_decoder')\n",
        "\n",
        "    if len(arq) == 3:\n",
        "        first_dense = Dense(arq[1], activation=\"linear\")(latent_inputs)\n",
        "\n",
        "        second_dense = Dense(arq[0], activation=\"linear\")(first_dense)\n",
        "\n",
        "        embedding_outputs = Dense(embedding_dim, activation=\"linear\")(second_dense)\n",
        "\n",
        "        density_outputs = Dense(density_dim, activation=\"linear\")(second_dense)\n",
        "\n",
        "        liwc_outputs = Dense(liwc_dim, activation=\"linear\")(second_dense)\n",
        "\n",
        "    elif len(arq) == 2:\n",
        "        first_dense = Dense(arq[0], activation=\"linear\")(latent_inputs)\n",
        "\n",
        "        embedding_outputs = Dense(embedding_dim, activation=\"linear\")(first_dense)\n",
        "\n",
        "        density_outputs = Dense(density_dim, activation=\"linear\")(first_dense)\n",
        "\n",
        "        liwc_outputs = Dense(liwc_dim, activation=\"linear\")(first_dense)\n",
        "\n",
        "    else:  # len(arq) == 1\n",
        "        embedding_outputs = Dense(embedding_dim, activation=\"linear\")(latent_inputs)\n",
        "\n",
        "        density_outputs = Dense(density_dim, activation=\"linear\")(latent_inputs)\n",
        "\n",
        "        liwc_outputs = Dense(liwc_dim, activation=\"linear\")(latent_inputs)\n",
        "\n",
        "    decoder = keras.Model(latent_inputs, [embedding_outputs, density_outputs, liwc_outputs], name=\"decoder\")\n",
        "\n",
        "    return decoder\n",
        "\n",
        "\n",
        "def triplevae(arq, embedding_dim, density_dim, liwc_dim, operator):\n",
        "    encoder = encoder_tvae(arq, embedding_dim, density_dim, liwc_dim, operator)\n",
        "\n",
        "    decoder = decoder_tvae(arq, embedding_dim, density_dim, liwc_dim)\n",
        "\n",
        "    tvae = TVAE(encoder, decoder, embedding_dim, density_dim, liwc_dim)\n",
        "\n",
        "    tvae.compile(optimizer=keras.optimizers.Adam())\n",
        "\n",
        "    return tvae, encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0rxPYfUjROH"
      },
      "source": [
        "BERT = 'DistilBERT Multilingua'\n",
        "\n",
        "df_train = np.array(train[BERT].to_list())\n",
        "df_test = np.array(test[BERT].to_list())\n",
        "df_outlier = np.array(outlier[BERT].to_list())\n",
        "\n",
        "df_train_fet = np.array(train['features_normalized'].to_list()).astype('float32')\n",
        "df_test_fet = np.array(test['features_normalized'].to_list()).astype('float32')\n",
        "df_outlier_fet = np.array(outlier['features_normalized'].to_list()).astype('float32')\n",
        "\n",
        "cluster_list = [2, 4, 5]\n",
        "epoch = 10\n",
        "arq = [256]\n",
        "operator = 'max'\n",
        "\n",
        "density_train, density_test, density_outlier = make_density_information(cluster_list, df_train, df_test, df_outlier)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "tvae, encoderTVAE, decoder = triplevae(arq, len(df_train[0]), len(cluster_list), len(df_train_fet[0]), operator)\n",
        "\n",
        "tvae.fit([df_train, density_train, df_train_fet], [df_train, density_train, df_train_fet], epochs=epoch, batch_size=32, verbose=0)\n",
        "\n",
        "x_train, _, _ = encoderTVAE.predict([df_train, density_train, df_train_fet])\n",
        "x_test, _, _ = encoderTVAE.predict([df_test, density_test, df_test_fet])\n",
        "x_outlier, _, _ = encoderTVAE.predict([df_outlier, density_outlier, df_outlier_fet])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_73V9LralA9"
      },
      "source": [
        "# OCSVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_1pjNTIa0Mr"
      },
      "source": [
        "## Best parmeters of the representation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE-BQ4xHaze-"
      },
      "source": [
        "nu = 0.1\n",
        "gamma = 'scale'\n",
        "kernel= 'sigmoid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMtwbKrBnbos"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gHYq20wL4S"
      },
      "source": [
        "from sklearn.svm import OneClassSVM as OCSVM\n",
        "\n",
        "ocsvm = OCSVM(kernel=kernel,nu=nu,gamma=gamma)\n",
        "\n",
        "ocsvm.fit(x_train)\n",
        "\n",
        "y_pred_fake = ocsvm.predict(x_test)\n",
        "y_pred_true = ocsvm.predict(x_outlier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y2amEWayG6-",
        "outputId": "9a1aa1ef-fd85-40b5-cd0c-04a9aac88b94"
      },
      "source": [
        "print(evaluation_one_class(y_pred_fake,y_pred_true))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.89      0.88      0.88      1020\n",
            "           1       0.87      0.88      0.88       939\n",
            "\n",
            "    accuracy                           0.88      1959\n",
            "   macro avg       0.88      0.88      0.88      1959\n",
            "weighted avg       0.88      0.88      0.88      1959\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcN0N_q-bjJd"
      },
      "source": [
        "# Predict Proba Adaptation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh3BdDbZzkyK"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WuZXx0Qx225"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def representation(textual_documents):\n",
        "\n",
        "  if type(textual_documents) != list:\n",
        "    textual_documents = [textual_documents]\n",
        "  \n",
        "  if preproc == 'TripleVAE-BERT':\n",
        "    textual_documents_emb = sentence_embedding(textual_documents) # change the embeddings\n",
        "    \n",
        "    if on_test:\n",
        "      density = len(textual_documents) * [density_test[idx]] # congeal the test density information\n",
        "      liwc_rep = len(textual_documents) * [df_test_fet[idx]] # congeal the test liwc\n",
        "    else:\n",
        "      density = len(textual_documents) * [density_outlier[idx]] # congeal the outlier density information\n",
        "      liwc_rep = len(textual_documents) * [df_outlier_fet[idx]] # congeal the test liwc\n",
        "\n",
        "    textual_documents_vec,_,_ = encoderTVAE.predict([textual_documents_emb, np.array(density), np.array(liwc_rep)])\n",
        "  \n",
        "  elif preproc == 'TripleVAE-Density':\n",
        "    textual_documents_emb = sentence_embedding(textual_documents)\n",
        "\n",
        "    density = return_density_inf(df_train, textual_documents_emb) # change the density information\n",
        "    \n",
        "    if on_test:\n",
        "      embedding_lol = [df_test[idx]] * len(textual_documents) # congeal the test embeddings\n",
        "      liwc_rep = len(textual_documents) * [df_test_fet[idx]] # congeal the test liwc\n",
        "    else:\n",
        "      embedding_lol = [df_outlier[idx]] * len(textual_documents) # congeal the outlier embeddings\n",
        "      liwc_rep = len(textual_documents) * [df_outlier_fet[idx]] # congeal the test liwc\n",
        "\n",
        "    textual_documents_vec,_,_ = encoderTVAE.predict([np.array(embedding_lol), np.array(density), np.array(liwc_rep)])\n",
        "\n",
        "  elif preproc == 'TripleVAE-LIWC':\n",
        "    liwc_rep = return_LIWC(textual_documents)  # change the liwc\n",
        "    \n",
        "    if on_test:\n",
        "      embedding_lol = [df_test[idx]] * len(textual_documents) # congeal the test embeddings\n",
        "      density = len(textual_documents) * [density_test[idx]] # congeal the test density information\n",
        "    else:\n",
        "      embedding_lol = [df_outlier[idx]] * len(textual_documents) # congeal the outlier embeddings\n",
        "      density = len(textual_documents) * [density_outlier[idx]] # congeal the outlier density information\n",
        "\n",
        "    textual_documents_vec,_,_ = encoderTVAE.predict([np.array(embedding_lol), np.array(density), np.array(liwc_rep)])\n",
        "\n",
        "  return textual_documents_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhGF4mOu6ZmI"
      },
      "source": [
        "def normalize_decision_function(minmax_less, minmax_geq, list_decision_function):\n",
        "  \n",
        "  list_decision_function_normalize = []\n",
        "  \n",
        "  for value_decision_function in list_decision_function:\n",
        "    if value_decision_function < 0:\n",
        "      list_decision_function_normalize.append(minmax_less.transform([[value_decision_function]])[0][0])\n",
        "    else:\n",
        "      list_decision_function_normalize.append(minmax_geq.transform([[value_decision_function]])[0][0])\n",
        "\n",
        "  return list_decision_function_normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QohTdzbEww9O"
      },
      "source": [
        "def one_class_predict_proba(new_vecs):\n",
        "  \n",
        "  train_dec_fun = ocsvm.decision_function(x_train)\n",
        "  test_dec_fun = ocsvm.decision_function(x_test)\n",
        "  out_dec_fun = ocsvm.decision_function(x_outlier)\n",
        "  news_dec_fun = ocsvm.decision_function(new_vecs)\n",
        "\n",
        "  total = np.concatenate([train_dec_fun,test_dec_fun,out_dec_fun, news_dec_fun])\n",
        "\n",
        "  total_geq = total[total >= 0]\n",
        "  total_less = total[total < 0]\n",
        "\n",
        "  minmax_geq = MinMaxScaler(feature_range=(0.5,1)).fit(total_geq.reshape(-1, 1))\n",
        "  minmax_less = MinMaxScaler(feature_range=(0,0.5)).fit(total_less.reshape(-1, 1))\n",
        "  \n",
        "  list_decision_function_normalize = normalize_decision_function(minmax_less, minmax_geq, news_dec_fun)\n",
        "\n",
        "  list_predict_proba = []\n",
        "\n",
        "  for num in list_decision_function_normalize:\n",
        "    list_predict_proba.append(np.array([num,1-num]))\n",
        "\n",
        "  return np.array(list_predict_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNyKMSH9w-lX"
      },
      "source": [
        "def predict_proba(textual_documents):\n",
        "\n",
        "  new_vecs = representation(textual_documents)\n",
        "\n",
        "  list_predict_proba = one_class_predict_proba(new_vecs)\n",
        "\n",
        "  return list_predict_proba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFjMoQ2Xbyna"
      },
      "source": [
        "# Multimodal LIME for One-Class Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l7Vjo7DdIRH",
        "outputId": "218faed9-b763-4f79-c6e8-cff579253b0f"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 275 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (1.0.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.18.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (3.0.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=070ea929373fe8a01ab03670012fee8e652ea7c4054e8cc4bd19309d7a149d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFCs95-3vP6N"
      },
      "source": [
        "import lime\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import sklearn.metrics\n",
        "from __future__ import print_function\n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd13bOq8LCxj"
      },
      "source": [
        "explainer = LimeTextExplainer(class_names=['Fake','Real'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmHnKhOpNzWl"
      },
      "source": [
        "## Density\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiqaSvptCmv3"
      },
      "source": [
        "preproc = 'TripleVAE-Density'\n",
        "\n",
        "idx = 796\n",
        "\n",
        "on_test = True\n",
        "\n",
        "exp = explainer.explain_instance(test['text'].iloc[idx], predict_proba, num_features=10)\n",
        "print('Document id: %d' % idx)\n",
        "print('Probability(fake/true) =', predict_proba([test['text'].iloc[idx]])[0])\n",
        "print('True class: %s' % test['class'].iloc[idx])\n",
        "exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVL5iCpzjnpu"
      },
      "source": [
        "exp.save_to_file('lime-tvae-density.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeDWxIETN3Mp"
      },
      "source": [
        "## DBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V2PbVp-cXAM"
      },
      "source": [
        "preproc = 'TripleVAE-BERT'\n",
        "\n",
        "idx = 796\n",
        "\n",
        "on_test = True\n",
        "\n",
        "exp = explainer.explain_instance(test['text'].iloc[idx], predict_proba, num_features=10)\n",
        "print('Document id: %d' % idx)\n",
        "print('Probability(fake/true) =', predict_proba([test['text'].iloc[idx]])[0])\n",
        "print('True class: %s' % test['class'].iloc[idx])\n",
        "exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO0RQNeWj_Un"
      },
      "source": [
        "exp.save_to_file('lime-tvae-dbertml.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeSZdPW1N6sz"
      },
      "source": [
        "## LIWC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3340T-c4uPva"
      },
      "source": [
        "preproc = 'TripleVAE-LIWC'\n",
        "\n",
        "idx = 796\n",
        "\n",
        "on_test = True\n",
        "\n",
        "\n",
        "exp = explainer.explain_instance(test['text'].iloc[idx], predict_proba, num_features=10)\n",
        "print('Document id: %d' % idx)\n",
        "print('Probability(fake/true) =', predict_proba([test['text'].iloc[idx]])[0])\n",
        "print('True class: %s' % test['class'].iloc[idx])\n",
        "exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJSsECzYkDO8"
      },
      "source": [
        "exp.save_to_file('lime-tvae-liwc.html')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}